{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string, re\n",
    "from autocorrect import spell\n",
    "from nltk.tokenize.nist import NISTTokenizer\n",
    "tknzr = NISTTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "RE_D = re.compile('\\d')\n",
    "RE_AP = re.compile(\"'\")\n",
    "RE_DO = re.compile(\"\\.\")\n",
    "\n",
    "import time\n",
    "from timeout import timeout\n",
    "import pickle\n",
    "from math import pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x,n=3):\n",
    "\ty = pow(10,n)\n",
    "\treturn int(x*y)/pow(10,n)\n",
    "\n",
    "p = {}\n",
    "for i in string.punctuation:\n",
    "    p[i]=True\n",
    "def replace(s_out):\n",
    "    s1_out = []\n",
    "    for i in s_out:\n",
    "        try:\n",
    "            if(p[i]==True):\n",
    "                continue\n",
    "        except:\n",
    "            s1_out.append(i)\n",
    "    return s1_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('RateMDs.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt off-the-shelf Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#print(stop_words)\n",
    "to_keep = ['she','he','not','during','before','after','him','her','his','hers','herself','himself']\n",
    "to_remove = ['also',\"i'm\",\"i'll\",\"we've\",\"i've\",\"can't\",\"it's\",\"i'd\",\"you're\",\"doesn't\",\"that's\"\n",
    "             'got','would','many','some','few']\n",
    "for i in to_keep:\n",
    "    stop_words.remove(i)\n",
    "for i in to_remove:\n",
    "    stop_words.add(i)\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize. Preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "x = 0\n",
    "for r1 in r[start:]:\n",
    "#for r1 in r:\n",
    "    r1 = re.sub('\\s+', ' ', r1)\n",
    "    r_out = tknzr.tokenize(r1)   # or word_tokenize(r1)\n",
    "    r_out = replace(r_out)\n",
    "    for i in range(len(r_out)):\n",
    "        if r_out[i].lower() in stop_words:\n",
    "            r_out[i] = ''\n",
    "            continue\n",
    "        '''if r_out[i][0] > 'A' and r_out[i][0] < 'Z':\n",
    "            if r_out[i] != spell(r_out[i]):\n",
    "               print(r_out[i])\n",
    "        elif r_out[i] != spell(r_out[i]):\n",
    "            print(r_out[i])'''\n",
    "        if r_out[i][0] == 'x':\n",
    "            if spell(str(r_out[i])) == r_out[i]:\n",
    "                print(r_out[i],spell(r_out[i]))\n",
    "            else:\n",
    "                x+=1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "checker = enchant.Dict(\"en_US\")\n",
    "to_allow = ['xray','x-ray']\n",
    "digits = ['0','1','2','3','4','5','6','7','8','9']\n",
    "@timeout(5)\n",
    "def pre2(r1): # created June 19\n",
    "    r1 = r1.lower()\n",
    "    r1 = re.sub('\\s+', ' ', r1)\n",
    "    r_out = tknzr.tokenize(r1)   # or word_tokenize(r1)\n",
    "    r_out = replace(r_out)\n",
    "    for i in range(len(r_out)):\n",
    "        if len(r_out[i]) < 2:\n",
    "            r_out[i] = ''\n",
    "            continue\n",
    "        if r_out[i][0] in digits:\n",
    "            r_out[i] = ''\n",
    "            continue\n",
    "        if r_out[i] in to_allow:\n",
    "            continue\n",
    "        if checker.check(r_out[i]) == False:\n",
    "            r_out[i] = ''\n",
    "            continue\n",
    "        if r_out[i] in stop_words:\n",
    "            r_out[i] = ''\n",
    "            continue\n",
    "        if r_out[i] in names_ml:\n",
    "            r_out[i] = ''\n",
    "            continue\n",
    "        r_out[i] = lemmatizer.lemmatize(r_out[i])    \n",
    "\n",
    "    r_out = (' ').join(r_out)\n",
    "    r_out = r_out.strip()\n",
    "    r_out = r_out.lower()\n",
    "    r_out = re.sub('\\s+', ' ', r_out)\n",
    "    r_out = r_out.split(' ')\n",
    "    return r_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(text.values)\n",
    "print(len(r))\n",
    "print(r[0])\n",
    "print(type(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R = []\n",
    "start = time.time()\n",
    "timer1 = start\n",
    "len_r = len(r)\n",
    "errors = []\n",
    "for i,r1 in enumerate(r):\n",
    "    if(i%100==0):\n",
    "        timer2 = time.time()\n",
    "        print(i,norm(timer2-timer1))\n",
    "        timer1 = timer2\n",
    "    if(i%1000==0 and i != 0):\n",
    "        t_so_far = norm((timer2-start)/60,0)\n",
    "        print(norm(i/len_r)*100,'% done in ',t_so_far,' minutes\\t',norm((t_so_far*(len_r-i))/i,0),' minutes remaining...')\n",
    "    try:\n",
    "        r1 = r1[2:-1]\n",
    "        new2 = pre2(r1)\n",
    "        R.append(new2)\n",
    "    except:\n",
    "        print('\\t',i,r1)\n",
    "        errors.append(i)\n",
    "    \n",
    "timer2 = time.time()\n",
    "print(i,norm(timer2-timer1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(R),len(r),len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[errors[0],:]\n",
    "df.drop(df.index[errors],inplace=True)\n",
    "print(errors[0:10])\n",
    "print(len(df))\n",
    "#,len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.loc[errors[0],:]\n",
    "df['text'] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[172,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now cleaned. Let's pick the most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df['specialty'].value_counts().to_dict()\n",
    "import operator\n",
    "sorted_freq = sorted(freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1000\n",
    "allowed = [] # list of allowed specialties, which have frequency above threshold\n",
    "for i in sorted_freq:\n",
    "    if i[1] >= threshold:\n",
    "        allowed.append(i[0])\n",
    "    else:\n",
    "        break\n",
    "print(allowed)\n",
    "print(len(allowed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "ignores = []\n",
    "for i,s in enumerate(df['specialty']):\n",
    "    if(s in allowed):\n",
    "        try:\n",
    "            freq[s]+=1\n",
    "            if(freq[s]>threshold):\n",
    "                ignores.append(i)\n",
    "        except:\n",
    "            freq[s] = 1\n",
    "    else:\n",
    "        ignores.append(i)\n",
    "df.drop(df.index[ignores],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df),ignores[0],df.iloc[ignores[0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('subset.pickle',protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
