{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('subset.pickle','rb') as h:\n",
    "    data = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping specialties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_specialties = {\n",
    " 'internist-geriatrician':'int',\n",
    " 'gastroenterologist':'gas',\n",
    " 'family-gp':'family',\n",
    " 'pain-mgmt-physical-rehab':'bone',\n",
    " 'reproductive-endocrinologist':'gyn',\n",
    " 'orthopedics-sports':'bone',\n",
    " 'dermatologist':'skin',\n",
    " #'acupuncturist':'alt',\n",
    " #'naturopath':'alt',\n",
    " 'ophthalmologist':'eye',\n",
    " 'psychiatrist':'psych',\n",
    " 'cosmetic-plastic-surgeon':'skin',\n",
    " #'optometrist':'eye',\n",
    " #'chiropractor':'bone',\n",
    " 'pediatrician':'pedia',\n",
    " 'neurosurgeon':'brain',\n",
    " #'orthodontist':'dent',\n",
    " 'endocrinologist':'endo',\n",
    " 'neurologist':'brain',\n",
    " #'dentist':'dent',\n",
    " #'podiatrist':'bone',\n",
    " 'surgeon-general':'surg',\n",
    " 'ear-nose-and-throat-ent':'ent',\n",
    " 'gynecologist-obgyn':'gyn',\n",
    " #'psychologist':'psych',\n",
    " #'oral-surgeon':'dent'\n",
    "}\n",
    "\n",
    "for i in range(len(data)):\n",
    "    entry = data.iloc[i]\n",
    "    name = entry.name\n",
    "    try:\n",
    "        data.set_value(name,'specialty', group_specialties[entry['specialty']])\n",
    "    except:\n",
    "        data.set_value(name,'specialty', '-1')\n",
    "    if i%1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open('grouped.pickle','wb') as h:\n",
    "#    pickle.dump(data,h,protocol=2)\n",
    "with open('grouped.pickle','rb') as h:\n",
    "    data = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[10]\n",
    "df = data.groupby('doc_name').apply(lambda x: x.sum())\n",
    "df.iloc[1]\n",
    "df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also make a set of specialties and genders\n",
    "specialties, genders = set(), set()\n",
    "# make a dict of specialty and gender by doc name\n",
    "d = {}\n",
    "for i in range(len(data)):\n",
    "    doc = data.iloc[i]\n",
    "    s = doc['specialty']\n",
    "    g = doc['gender']\n",
    "    specialties.add(s)\n",
    "    genders.add(g)\n",
    "    d[doc['doc_name']] = [s,g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialties = list(specialties)\n",
    "specialties.remove('-1')\n",
    "specialties.append('-1') # HAHA! Now we've got -1 at the end, as default.\n",
    "s_map = {}\n",
    "for i,s in enumerate(specialties):\n",
    "    s_map[s] = i\n",
    "genders = list(genders)\n",
    "print(len(specialties),len(genders))\n",
    "genders\n",
    "specialties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintain vocabulary of required words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_freq = {}\n",
    "    \n",
    "for i in range(len(df)):\n",
    "    #if d[df.iloc[i].name][0] == '-1':\n",
    "    #   continue\n",
    "    for w in df.iloc[i]['text']:\n",
    "        try:\n",
    "            V_freq[w] += 1\n",
    "        except:\n",
    "            V_freq[w] = 1\n",
    "            \n",
    "import operator\n",
    "sorted_V_freq = sorted(V_freq.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_V[-500:-490]\n",
    "# let's arbitrarily trim it to 1000 words (total 14417 actually)\n",
    "vocab_size = 1000 # also experimented with 500\n",
    "\n",
    "V_map = {}\n",
    "V_new = sorted_V_freq[-vocab_size:]\n",
    "for i,tup in enumerate(V_new):\n",
    "    V_map[tup[0]] = i\n",
    "len(V_map)\n",
    "list(V_map.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = len(specialties)\n",
    "V = len(V_map)\n",
    "#size_of_var = 2+2*V+2*S+2*S*V\n",
    "size_of_var = 2+V+2*S+2*S*V # beta_w no longer needed.\n",
    "print(1,2,2+S,2+S+V,2+S+V+S,2+S+V+S+S*V,size_of_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize 3d array as 2d array\n",
    "y = {}\n",
    "for s in specialties:\n",
    "    y[s] = {'m':{},'f':{}}\n",
    "\n",
    "# dict to store (later do log of these values) doctor wise frequencies.\n",
    "V_beta_w = {}\n",
    "    \n",
    "# make set out of text aggregations\n",
    "for i in range(len(df)):\n",
    "    words = set()\n",
    "    entry = df.iloc[i]\n",
    "    if d[entry.name][0] == '-1':\n",
    "        continue\n",
    "    for w in entry['text']:\n",
    "        words.add(w)\n",
    "# and fill the y 3d matrix (y_sgw)\n",
    "    sg = d[entry.name]\n",
    "    for w in words:\n",
    "        if w not in V_map.keys():\n",
    "            continue\n",
    "        try:\n",
    "            V_beta_w[w] += 1\n",
    "        except:\n",
    "            V_beta_w[w] = 1\n",
    "        try:\n",
    "            y[sg[0]][sg[1]][w] += 1\n",
    "        except:\n",
    "            y[sg[0]][sg[1]][w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y['int']['m'])\n",
    "len(V_beta_w.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(s,g,w):\n",
    "    #var = [0] * size_of_var # list of vars.\n",
    "    var = bitarray(size_of_var)\n",
    "    var.setall(0)\n",
    "    \n",
    "    var[0] = 1\n",
    "    #var[2+V_map[w]] = 1\n",
    "    var[2+s_map[s]] = 1\n",
    "    if g == 'f':\n",
    "        var[1] = 1 # beta_g\n",
    "        var[2+S+V_map[w]] = 1 # beta_gw\n",
    "        var[2+S+V+s_map[s]] = 1 # beta_gs\n",
    "        var[2+S+V+S+S*V+ (s_map[s]*V) + (V_map[w])] = 1 # beta_sgw\n",
    "    var[2+S+V+S+ (s_map[s]*V) + (V_map[w]) ] = 1 # beta_sw\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitarray import bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size_of_var1 = 1+1+V+S # WARNING: V was used earlier too.\n",
    "def assign2(s,g,w):\n",
    "    #var = [0] * size_of_var1 # list of vars.\n",
    "    var = bitarray(size_of_var1)\n",
    "    var.setall(0)\n",
    "    #var = np.zeros(size_of_var1, dtype=bool)\n",
    "    #var = np.zeros(size_of_var1, dtype=int)\n",
    "    var[0] = 1\n",
    "    var[2+V_map[w]] = 1\n",
    "    var[2+V+s_map[s]] = 1\n",
    "    if g == 'f':\n",
    "        var[1] = 1\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "count = 0\n",
    "for s in specialties:\n",
    "    for g in genders:\n",
    "        for w in list(y[s][g].keys()):\n",
    "            count+=1\n",
    "            # take log, discount the log of general word frequency, i.e. no need to train beta_w.\n",
    "            y[s][g][w] = math.log(y[s][g][w]) - math.log(V_beta_w[w])\n",
    "count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['gyn']['f']['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "X,Y = [],[]\n",
    "for s in specialties:\n",
    "    for g in genders:\n",
    "        for w in list(y[s][g].keys()):\n",
    "            x = assign(s,g,w)\n",
    "            ysgw = y[s][g][w]\n",
    "            X.append(x)\n",
    "            Y.append(ysgw)\n",
    "            count+=1\n",
    "            if count%100 == 0:\n",
    "                print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "LR = {'X':X,'Y':Y}\n",
    "version = 'july23'\n",
    "with open('LR_'+version+'.pickle','wb') as h:\n",
    "    pickle.dump(LR,h,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = LR['X']\n",
    "Y = LR['Y']\n",
    "Y[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = len(specialties) # 13\n",
    "V = vocab_size # 1000\n",
    "len(X) # 11612\n",
    "2+S+V+S+V*S+((S)*V)-1 # 12525\n",
    "len(X[0]) # 12526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore some\n",
    "ig = [] # the indices to be ignored.\n",
    "ig.append(2+S-1)\n",
    "ig.append(2+S+V-1)\n",
    "#ig.append(2+S+V+S-1) counted in the i=0 next loop.\n",
    "for i in range(S):\n",
    "    ig.append(2+S+V+S+(i*V)-1)\n",
    "# 2+S+V+S+((S-1)*V)-1 counted.\n",
    "\n",
    "# If you don't want beta_sgw:\n",
    "for i in range(2+S+V+S+(S-1)*V-1,len(X[0])):\n",
    "    ig.append(i)\n",
    "'''for i in range(2+S+V+S+((S-1)*V)-1,2+S+V+S+((S)*V)-1): # starting one shouldnt have -1. just to test.\n",
    "    ig.append(i)\n",
    "for i in range(S): # 0 value counts the last one that should have been counted in loop above.\n",
    "    ig.append(2+S+V+S+V*S+(i*V)-1)\n",
    "for i in range(2+S+V+S+V*S+((S-1)*V)-1,2+S+V+S+V*S+((S)*V)-1): # starting one shouldnt have -1. just to test.\n",
    "    ig.append(i)'''\n",
    "# last element should be last element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to actually ignore some variables not needed.\n",
    "import numpy as np\n",
    "%time X1 = np.array(X)\n",
    "X1.shape\n",
    "%time X2 = np.delete(X1,ig,axis=1)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time regr.fit(X2,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version = 'july23'\n",
    "#with open('LR_model_'+version+'.pickle','wb') as h:\n",
    "#    pickle.dump(regr,h,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "version = 'july23'\n",
    "with open('LR_model_'+version+'.pickle','rb') as h:\n",
    "    regr = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = regr.coef_\n",
    "regr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[1] # FEMALE coefficient. beta_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[2:2+S] # SPECIALTIES coefficients. beta_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_w = {}\n",
    "for i,w in enumerate(c[2:2+V]):\n",
    "    beta_w[V_new[i]] = w\n",
    "sorted_beta_w = sorted(beta_w.items(), key=operator.itemgetter(1))\n",
    "sorted_beta_w[0:10]\n",
    "# almost, appreciate, putting ... most frequent words not otherwise compensated in beta_gw or beta_sw.\n",
    "# words that are neither gender-specific nor specialty-specific but are frequent nevertheless, appear here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_gw = {}\n",
    "for i,w in enumerate(c[2+S-1:2+S+V-2]):\n",
    "    beta_gw[V_new[i]] = w\n",
    "sorted_beta_gw = sorted(beta_gw.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_beta_gw[0:20]\n",
    "# him, he's, his... (Closest to 'm')\n",
    "sorted_beta_gw[-20:]\n",
    "# ... herself, podiatrist (Closest to 'f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_gs = {}\n",
    "for i,w in enumerate(c[2+S+V-2:2+S+V+S-3]):\n",
    "    beta_gs[specialties[i]] = w\n",
    "sorted_beta_gs = sorted(beta_gs.items(), key=operator.itemgetter(1))\n",
    "sorted_beta_gs\n",
    "# brain, surg, bone ... int, skin, family (Closer to 'm' towards... Closer to 'f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_sw = {}\n",
    "start = 2+S+V+S-3 # constant,gender, (skipped) beta_w, beta_s, beta_gw, beta_gs\n",
    "for i in range(S-1):\n",
    "    beta_sw_temp = {}\n",
    "    for j in range(V-1):\n",
    "        beta_sw_temp[V_new[j]] = c[start + j]\n",
    "    start += V-1\n",
    "    beta_sw[specialties[i]] = beta_sw_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_beta_sw = {}\n",
    "for i in specialties[:-1]:\n",
    "    sorted_beta_sw[i] = sorted(beta_sw[i].items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_beta_sw['skin'][-10:0]\n",
    "#tuple_word = list(beta_sw['skin'].keys())[0]\n",
    "#beta_sw['gyn'][tuple_word]\n",
    "#sorted_beta_sw['skin'][-1]\n",
    "for i in specialties[:-1]:\n",
    "    print(\" - \"+i+\":\")\n",
    "    for j in sorted_beta_sw[i][-10:]:\n",
    "        print(j)\n",
    "    print('\\n')\n",
    "# Individual specialties' nearest words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
